apiVersion: v1
data:
  workers.yaml: |2

    - name: agent_zero_trainer
      versions: ["20.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: classification_trainer
      versions: ["20.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: clustering_trainer
      versions: ["20.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: clustering_updater
      versions: ["20.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: data_analysis_trainer
      versions: ["20.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: data_analysis_updater
      versions: ["20.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: embedding_trainer
      versions: ["20.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: fuzzy_matcher_trainer
      versions: ["20.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: nlu_trainer
      versions: ["20.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: promin_interactive_trainer
      versions: ["20.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: promin_show_record_trainer
      versions: ["22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: promin_trainer
      versions: ["1.0.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: regression_trainer
      versions: ["20.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: search_relevancy_trainer
      versions: ["20.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: similarity_trainer
      versions: ["20.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
    - name: tree_similarity_trainer
      versions: ["20.0","21.0","22.0","23.0","24.0","25.0"]
      job:
        spec:
          # Automatic cleanup of finished jobs.
          ttlSecondsAfterFinished: 300
          # Job retry is handled by Glide.
          # Setting this to zero so the pod is not retried in case a container fails.
          backoffLimit: 0
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: trainer
                splunk.com/index: mltrainer
            spec:
              # Job retry is handled by Glide.
              restartPolicy: Never
              # Allow training jobs on GPU nodes.
              tolerations:
                - effect: NoSchedule
                  key: node.servicenow.com/dedicated
                  operator: Equal
                  value: ml-workload-only
              volumes:
                - name: root-ca
                  secret:
                    secretName: ml-worker-agent-root-ca
              containers:
                - name: trainer
                  image: devsnc-docker-dev-remote.artifactory.servicenow.net/devsnc/ml-java-trainer-job-trainer-job-snapshot:25.1-JM23
                  env:
                    - name: ML_SERVER_RUN_MODE
                      value: JOB
                    - name: ML_RESOURCES_LIMITS_CPU
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.cpu
                    - name: ML_RESOURCES_LIMITS_MEMORY
                      valueFrom:
                        resourceFieldRef:
                          containerName: trainer
                          resource: limits.memory
                    - name: ML_SCHEDULER_URL
                      value: https://k8slabmtifa.service-now.com/
                    - name: REQUESTS_CA_BUNDLE
                      value: "/etc/root-ca/tls-ca-bundle.pem"
                  envFrom:
                    - secretRef:
                        name: ml-worker-agent-basic-auth
                  volumeMounts:
                    - name: root-ca
                      readOnly: true
                      mountPath: "/etc/root-ca"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
                    limits:
                      memory: "16Gi"
                      cpu: "8"
                      ephemeral-storage: "20Gi"
kind: ConfigMap
metadata:
  annotations:
    meta.helm.sh/release-name: ml-worker-agent
    meta.helm.sh/release-namespace: ml-trainers-dev-java
  creationTimestamp: "2023-11-10T08:30:53Z"
  labels:
    app.kubernetes.io/instance: ml-worker-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ml-worker-agent
    app.kubernetes.io/version: 24.1.4
    git-ref: release-lab
    helm.sh/chart: ml-worker-agent-1.0.0
  name: ml-worker-agent-workers
  namespace: ml-trainers-dev-java
  resourceVersion: "380966110"